{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from core.text_detection import DB_text_detection\n",
    "from core.text_detection import EAST_text_detection\n",
    "from core.segmentation import char_segmentation\n",
    "from config.config_helpers import Config\n",
    "from core.segmentation import word_segmentation\n",
    "\n",
    "from utils.image_utils.Image import showImage\n",
    "from utils.image_utils.Image import Image_preprocessing\n",
    "from utils.image_utils.Image import image_padding\n",
    "from utils.image_utils.Image import simple_threshold\n",
    "from utils.image_utils import warp_image_crop\n",
    "\n",
    "image_path = f\"datasets\\\\sample_data\\\\tx.png\"\n",
    "json_config = Config(f\"config\\\\config.json\")\n",
    "DB_model_path = f\"models\\\\{json_config.get('image.DB_detection_model')}\"\n",
    "EAST_model_path = f\"models\\\\{json_config.get('image.EAST_detection_model')}\"\n",
    "\n",
    "image = cv.imread(image_path)\n",
    "frame = Image_preprocessing(image, json_config.get('image.scaling_factor'))\n",
    "DB_frame = DB_text_detection(frame._thresh, DB_model_path)\n",
    "EAST_frame = EAST_text_detection(frame._thresh, EAST_model_path)\n",
    "\n",
    "output_size = json_config.get('image.output_image_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showImage(frame._thresh)\n",
    "# print(json_config.get('image.EAST_detection_model'))\n",
    "\n",
    "# showImage(DB_frame._bound_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m croppedLineImage \u001b[38;5;241m=\u001b[39m warp_image_crop\u001b[38;5;241m.\u001b[39mcrop_image(frame\u001b[38;5;241m.\u001b[39m_default_cvImage, boxes)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# showImage(croppedLineImage[0])\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# word = word_segmentation(croppedLineImage[1])\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# showImage(word._cropped_image[3])\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m char \u001b[38;5;241m=\u001b[39m char_segmentation(\u001b[43mword\u001b[49m\u001b[38;5;241m.\u001b[39m_cropped_image[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     10\u001b[0m showImage(char\u001b[38;5;241m.\u001b[39m_bound_image)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word' is not defined"
     ]
    }
   ],
   "source": [
    "boxes, confidences = warp_image_crop.sort_boxes(DB_frame._boxes, DB_frame._confidence)\n",
    "croppedLineImage = warp_image_crop.crop_image(frame._default_cvImage, boxes)\n",
    "# showImage(croppedLineImage[0])\n",
    "\n",
    "# word = word_segmentation(croppedLineImage[1])\n",
    "# showImage(word._cropped_image[3])\n",
    "\n",
    "char = char_segmentation(word._cropped_image[0])\n",
    "\n",
    "showImage(char._bound_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data loader function definition\n",
    "# predict_list = []\n",
    "# char_list = []\n",
    "# al = 0\n",
    "# # baris ke kata\n",
    "# for lines in croppedLineImage:\n",
    "#     wordImage = word_segmentation(lines)\n",
    "#     # print(f\"ada {len(wordImage._cropped_image)} kata\")\n",
    "\n",
    "#     word_list = []\n",
    "\n",
    "#     # error handling for empty char detection\n",
    "#     if wordImage._cropped_image:\n",
    "#         for word in wordImage._cropped_image:\n",
    "#             charImage = char_segmentation(word)\n",
    "\n",
    "#             padding_list = []\n",
    "\n",
    "#             if charImage._mask:\n",
    "#                 for mask in charImage._mask:\n",
    "#                     mask = np.clip(mask, 0, 255).astype(np.uint8)\n",
    "#                     padded_image = image_padding(mask, image_size=(output_size, output_size), interpolation=0)\n",
    "#                     padding_list.append(padded_image)\n",
    "\n",
    "#                     # charlist space app\n",
    "#                     char_list.append('char')\n",
    "\n",
    "#             word_list.append(padding_list)\n",
    "\n",
    "#             # charlist space\n",
    "#             char_list.append('space')\n",
    "\n",
    "#     predict_list.append(word_list)\n",
    "#     char_list.append('break')\n",
    "\n",
    "# display_image = predict_list[0][3][2]\n",
    "# plt.imshow(display_image, cmap='gray')\n",
    "# print(display_image.shape)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader function definition\n",
    "char_list = []\n",
    "for lines in croppedLineImage:\n",
    "    wordImage = word_segmentation(lines)\n",
    "\n",
    "    # error handling for empty char detection\n",
    "    if wordImage._cropped_image:\n",
    "        for index, word in enumerate(wordImage._cropped_image):\n",
    "            charImage = char_segmentation(word)\n",
    "\n",
    "            if charImage._mask:\n",
    "                for mask in charImage._mask:\n",
    "                    mask = np.clip(mask, 0, 255).astype(np.uint8)\n",
    "                    padded_image = image_padding(mask, image_size=(output_size, output_size), interpolation=0)\n",
    "\n",
    "                    # charlist\n",
    "                    char_list.append(['char', padded_image])\n",
    "\n",
    "            # charlist\n",
    "            if index == len(wordImage._cropped_image) - 1:\n",
    "                pass\n",
    "            else:\n",
    "                char_list.append(['space', ' '])\n",
    "\n",
    "    char_list.append(['break', '\\n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] loading model from model_path: models\\ResNet\\Chars74K_ResNet50.keras\n",
      "[info] creating batch dataset\n",
      "[info] batch dataset created\n",
      "[info] len of batches is 3, with size per batch: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing batches:  33%|███▎      | 1/3 [00:04<00:08,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/3 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing batches:  67%|██████▋   | 2/3 [00:04<00:02,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2/3 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing batches: 100%|██████████| 3/3 [00:07<00:00,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3/3 processed\n",
      "[info] result time for batch prediction: 7.88 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from core.char import char_batch_predict\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# json_config.get('models.ResNet')[0] for model ResNet50\n",
    "# json_config.get('models.ResNet')[1] for model ResNet101\n",
    "char_recognition_model_path = f\"models\\\\ResNet\\\\{json_config.get('models.ResNet')[0]}\"\n",
    "class_names = json_config.get('models.label')\n",
    "\n",
    "char = char_batch_predict(char_recognition_model_path, class_names, char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It waS Che beSt Of\n",
      "timcS1 it waS Chc wOrSt\n",
      "Of cimeS1 it waS che agc\n",
      "Of wiSdOm1 it waS che\n",
      "agc Of fOO1iShncSS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(char._text_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
